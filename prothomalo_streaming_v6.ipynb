{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc32979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prothomalo_streaming_v6.ipynb\n",
    "# - Thread-safe state saves (locks) to fix \"dictionary changed size during iteration\"\n",
    "# - Auto-resume, auto-unwedge, and tidy heartbeat (last-line only)\n",
    "# - Month->day fallback to avoid 10k window caps\n",
    "# - Continues from existing JSONL (seeds seen_urls from saved articles)\n",
    "# - IP-ban backoff (1 hour), memory friendly queue, and periodic compaction\n",
    "# - Always safe to stop/restart; picks up from state\n",
    "\n",
    "import os, sys, json, time, math, random, threading, queue, gzip, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ------------------- User paths (edit if you want) -------------------\n",
    "BASE_DIR        = Path.home() / \"Downloads\"\n",
    "ARTICLES_JL     = BASE_DIR / \"prothomalo_articles.jsonl\"\n",
    "LINKS_TXT       = BASE_DIR / \"prothomalo_links.txt\"     # optional, not required for streaming\n",
    "STATE_JSON      = BASE_DIR / \"prothomalo_stream_state.json\"\n",
    "SESSION_JSON    = BASE_DIR / \"prothomalo_session_stats.json\"  # rotating session stats\n",
    "\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------- Controls -------------------\n",
    "SECTION_IDS = (\n",
    "    \"17532,17533,17535,17536,17538,17552,17553,17555,17556,17560,17562,17563,17566,17567,17568,17569,17570,17571,17572,\"\n",
    "    \"17573,17584,17585,17586,17587,17588,17589,17591,17599,17600,17602,17606,17678,17679,17680,17681,17682,17683,17684,\"\n",
    "    \"17685,17686,17687,17688,17689,17690,17691,17693,17694,17695,17696,17697,17698,17699,17700,17701,17702,17704,17705,\"\n",
    "    \"17706,17708,17709,17714,17717,17736,17737,17738,17739,17743,19182,19183,19184,19185,19195,19196,19197,19198,19199,\"\n",
    "    \"19200,22236,22237,22321,22323,22324,22325,22326,22327,22328,22329,22330,22332,22333,22334,22335,22336,22337,22338,\"\n",
    "    \"22339,22340,22341,22342,22349,22350,22351,22352,22362,22363,22364,22365,22368,22515,22516,22517,22518,22519,22520,\"\n",
    "    \"22575,22701,23230,23382,23383,23426,24541,26653,29465,35621,35622,35623,35624,35625,35626,35867,35868,35871,67467,95322\"\n",
    ")\n",
    "\n",
    "SEARCH_URL = \"https://www.prothomalo.com/search\"\n",
    "\n",
    "# Producer/consumer tuning\n",
    "LIMIT_PER_CALL      = 200\n",
    "QUEUE_MAXSIZE       = 5000\n",
    "PRODUCER_SLEEP_BASE = (0.1, 0.2)   # jitter between search calls\n",
    "CONSUMER_SLEEP_BASE = (0.1, 0.2)   # jitter between article calls\n",
    "N_CONSUMERS         = 50\n",
    "\n",
    "# Backoff\n",
    "IPBAN_BACKOFF_SECS  = 3600  # 1 hour\n",
    "RETRY_BACKOFFS      = [1, 2, 4, 8, 16, 32]\n",
    "\n",
    "# Save/ETA\n",
    "SAVE_EVERY_N        = 50\n",
    "HEARTBEAT_EVERY_SEC = 3\n",
    "\n",
    "# HTML fetch timeouts\n",
    "REQ_TIMEOUT         = (10, 20)  # connect, read\n",
    "\n",
    "# Global locks/state\n",
    "STATE_LOCK = threading.RLock()\n",
    "SESSION_LOCK = threading.RLock()\n",
    "\n",
    "# Session counters (not persisted, for nice heartbeat only)\n",
    "SESSION = {\n",
    "    \"produced\": 0,\n",
    "    \"consumed\": 0,\n",
    "    \"queue_max\": 0,\n",
    "    \"start_ts\": time.time(),\n",
    "    \"last_rate\": 0.0,\n",
    "}\n",
    "\n",
    "def now_utc():\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "def to_epoch_ms(dt: datetime) -> int:\n",
    "    return int(dt.timestamp() * 1000)\n",
    "\n",
    "def month_windows_until(snapshot_utc: str, months_back=307):\n",
    "    \"Newest->oldest months up to snapshot_utc.\"\n",
    "    snap = datetime.fromisoformat(snapshot_utc.replace(\"Z\",\"+00:00\"))\n",
    "    snap = snap.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    windows = []\n",
    "    cur_end = snap + timedelta(days=1)  # exclusive end of last day\n",
    "    for _ in range(months_back):\n",
    "        m_start = cur_end.replace(day=1)\n",
    "        prev_end = m_start\n",
    "        m_prev_start = (m_start - timedelta(days=1)).replace(day=1)\n",
    "        windows.append((m_prev_start, prev_end))\n",
    "        cur_end = m_prev_start\n",
    "    return windows  # newest->oldest\n",
    "\n",
    "def day_slices(month_start: datetime, month_end: datetime):\n",
    "    \"Return list of (start,end) days newest->oldest inside a month window.\"\n",
    "    days = []\n",
    "    d = month_end\n",
    "    while d > month_start:\n",
    "        d0 = (d - timedelta(days=1))\n",
    "        days.append((d0, d))\n",
    "        d = d0\n",
    "    return days  # newest->oldest\n",
    "\n",
    "def init_state_defaults(st: dict) -> dict:\n",
    "    \"Ensure new keys exist for old states.\"\n",
    "    with STATE_LOCK:\n",
    "        st.setdefault(\"snapshot_utc\", now_utc().isoformat().replace(\"+00:00\",\"Z\"))\n",
    "        st.setdefault(\"mode\", \"month\")       # \"month\" or \"day\"\n",
    "        st.setdefault(\"month_cursor\", None)  # ISO\n",
    "        st.setdefault(\"day_cursor\", None)    # ISO\n",
    "        st.setdefault(\"offset\", 0)\n",
    "        st.setdefault(\"seen_urls\", {})       # dict as tiny set\n",
    "        st.setdefault(\"ip_ban_until\", None)\n",
    "        st.setdefault(\"produced\", 0)\n",
    "        st.setdefault(\"consumed\", 0)\n",
    "        st.setdefault(\"last_save_ts\", time.time())\n",
    "    return st\n",
    "\n",
    "def load_done_set():\n",
    "    s = set()\n",
    "    if ARTICLES_JL.exists():\n",
    "        with open(ARTICLES_JL, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                if not line: continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    url = obj.get(\"url\")\n",
    "                    if url: s.add(url)\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return s\n",
    "\n",
    "def save_state(st: dict):\n",
    "    \"Atomic, thread-safe state save. Locks during dump to avoid dict-size errors.\"\n",
    "    with STATE_LOCK:\n",
    "        tmp = STATE_JSON.with_suffix(\".tmp\")\n",
    "        with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(st, f, ensure_ascii=False, indent=2)\n",
    "        tmp.replace(STATE_JSON)\n",
    "\n",
    "def load_state():\n",
    "    if STATE_JSON.exists():\n",
    "        with open(STATE_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            st = json.load(f)\n",
    "    else:\n",
    "        st = {}\n",
    "    return init_state_defaults(st)\n",
    "\n",
    "def persist_session_snapshot():\n",
    "    with SESSION_LOCK:\n",
    "        data = dict(SESSION)\n",
    "    try:\n",
    "        with open(SESSION_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def fetch_json(session, params):\n",
    "    \"Search API wrapper with simple jitter/backoff and ip-ban awareness.\"\n",
    "    url = SEARCH_URL\n",
    "    for attempt, back in enumerate([0]+RETRY_BACKOFFS):\n",
    "        time.sleep(random.uniform(*PRODUCER_SLEEP_BASE) + back)\n",
    "        with STATE_LOCK:\n",
    "            ipban_until = load_state().get(\"ip_ban_until\")\n",
    "        if ipban_until:\n",
    "            until = datetime.fromisoformat(ipban_until)\n",
    "            if now_utc() < until:\n",
    "                sleep_for = (until - now_utc()).total_seconds()\n",
    "                time.sleep(max(5, min(sleep_for, 120)))\n",
    "                continue\n",
    "        try:\n",
    "            r = session.get(url, params=params, timeout=REQ_TIMEOUT)\n",
    "            if r.status_code in (403, 429):\n",
    "                with STATE_LOCK:\n",
    "                    st = load_state()\n",
    "                    st[\"ip_ban_until\"] = (now_utc() + timedelta(seconds=IPBAN_BACKOFF_SECS)).isoformat().replace(\"+00:00\",\"Z\")\n",
    "                    save_state(st)\n",
    "                return None, r.status_code\n",
    "            if r.status_code != 200:\n",
    "                continue\n",
    "            return r.text, 200\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "    return None, 599\n",
    "\n",
    "def parse_search_links(html: str):\n",
    "    \"Extract article links from a search page HTML.\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if not href: continue\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://www.prothomalo.com\" + href\n",
    "        if re.match(r\"^https?://www\\.prothomalo\\.com/.+/[A-Za-z0-9]+$\", href):\n",
    "            links.append(href)\n",
    "    seen = set(); out=[]\n",
    "    for u in links:\n",
    "        if u not in seen:\n",
    "            seen.add(u); out.append(u)\n",
    "    return out\n",
    "\n",
    "def fetch_html(session, url):\n",
    "    for attempt, back in enumerate([0]+RETRY_BACKOFFS):\n",
    "        time.sleep(random.uniform(*CONSUMER_SLEEP_BASE) + back)\n",
    "        with STATE_LOCK:\n",
    "            ipban_until = load_state().get(\"ip_ban_until\")\n",
    "        if ipban_until:\n",
    "            until = datetime.fromisoformat(ipban_until)\n",
    "            if now_utc() < until:\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "        try:\n",
    "            r = session.get(url, timeout=REQ_TIMEOUT)\n",
    "            if r.status_code in (403, 429):\n",
    "                with STATE_LOCK:\n",
    "                    st = load_state()\n",
    "                    st[\"ip_ban_until\"] = (now_utc() + timedelta(seconds=IPBAN_BACKOFF_SECS)).isoformat().replace(\"+00:00\",\"Z\")\n",
    "                    save_state(st)\n",
    "                return None\n",
    "            if r.status_code != 200:\n",
    "                continue\n",
    "            return r.text\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def parse_article(html, url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    h1 = soup.find(\"h1\")\n",
    "    title = None\n",
    "    if h1:\n",
    "        title = h1.get(\"data-title-0\") or h1.get_text(strip=True)\n",
    "    section = None\n",
    "    sec_a = soup.find(\"a\", {\"data-section-0\": True})\n",
    "    if sec_a:\n",
    "        section = sec_a.get(\"data-section-0\") or sec_a.get_text(strip=True)\n",
    "    author = None\n",
    "    au_span = soup.find(\"span\", class_=re.compile(r\"contributor-name\"))\n",
    "    if au_span:\n",
    "        author = au_span.get(\"data-author-0\") or au_span.get_text(strip=True)\n",
    "    published_iso = None; published_text = None\n",
    "    t = soup.find(\"time\")\n",
    "    if t:\n",
    "        published_iso = t.get(\"datetime\")\n",
    "        published_text = t.get_text(\" \", strip=True)\n",
    "    tags = []\n",
    "    tag_ul = soup.find(\"ul\", class_=re.compile(r\"tag-list\"))\n",
    "    if tag_ul:\n",
    "        for a in tag_ul.find_all(\"a\", href=True):\n",
    "            txt = a.get_text(strip=True)\n",
    "            if txt: tags.append(txt)\n",
    "    body_parts = []\n",
    "    for div in soup.find_all(\"div\", class_=re.compile(r\"story-element-text\")):\n",
    "        for p in div.find_all(\"p\"):\n",
    "            txt = p.get_text()\n",
    "            if txt: body_parts.append(txt.strip())\n",
    "    if not body_parts:\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            txt = p.get_text()\n",
    "            if txt: body_parts.append(txt.strip())\n",
    "    body = \"\\n\\n\".join(body_parts).strip() if body_parts else None\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"section\": section,\n",
    "        \"author\": author,\n",
    "        \"published_iso\": published_iso,\n",
    "        \"published_text\": published_text,\n",
    "        \"tags\": tags,\n",
    "        \"body\": body,\n",
    "    }\n",
    "\n",
    "def fmt_eta(seconds):\n",
    "    if not seconds or seconds < 1: return \"0s\"\n",
    "    m, s = divmod(int(seconds), 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    if h: return f\"{h}h{m}m\"\n",
    "    if m: return f\"{m}m{s}s\"\n",
    "    return f\"{s}s\"\n",
    "\n",
    "def heartbeat_line(mode, st, qsize):\n",
    "    with STATE_LOCK:\n",
    "        produced = st.get(\"produced\",0)\n",
    "        consumed = st.get(\"consumed\",0)\n",
    "        ipSess = st.get(\"offset\",0)\n",
    "        ipban = st.get(\"ip_ban_until\")\n",
    "    with SESSION_LOCK:\n",
    "        elapsed = max(1e-6, time.time()-SESSION[\"start_ts\"])\n",
    "        rate = (consumed/elapsed)*60.0\n",
    "        SESSION[\"last_rate\"] = rate\n",
    "        SESSION[\"queue_max\"] = max(SESSION[\"queue_max\"], qsize)\n",
    "    eta = fmt_eta(qsize / max(rate, 1e-3) * 60.0) if qsize>0 else \"0s\"\n",
    "    base = f\"[Heartbeat] mode={mode} q={qsize} produced={produced} consumed={consumed} ipSess={ipSess} | avg={rate:.1f} items/min | ETA(queue) ~{eta}\"\n",
    "    if ipban:\n",
    "        base += \" | IP-BAN active\"\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b134fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def producer(work_q: queue.Queue, st: dict):\n",
    "    sess = requests.Session()\n",
    "    while True:\n",
    "        with STATE_LOCK:\n",
    "            mode = st.get(\"mode\",\"month\")\n",
    "            snapshot = st[\"snapshot_utc\"]\n",
    "            offset = st.get(\"offset\",0)\n",
    "            month_cursor = st.get(\"month_cursor\")\n",
    "            day_cursor = st.get(\"day_cursor\")\n",
    "        # Determine window\n",
    "        if mode == \"month\":\n",
    "            months = month_windows_until(snapshot, months_back=307)\n",
    "            if month_cursor is None:\n",
    "                m_start, m_end = months[0]\n",
    "            else:\n",
    "                mc = datetime.fromisoformat(month_cursor)\n",
    "                idx = next((i for i,(s,e) in enumerate(months) if e == mc), 0)\n",
    "                m_start, m_end = months[idx]\n",
    "            win_start, win_end = m_start, m_end\n",
    "        elif mode == \"done\":\n",
    "            break\n",
    "        else:\n",
    "            m_end = datetime.fromisoformat(st[\"month_cursor\"])\n",
    "            m_start = (m_end - timedelta(days=1)).replace(day=1)\n",
    "            days = day_slices(m_start, m_end)\n",
    "            if day_cursor is None:\n",
    "                d_start, d_end = days[0]\n",
    "            else:\n",
    "                d_end = datetime.fromisoformat(day_cursor)\n",
    "                idx = next((i for i,(s,e) in enumerate(days) if e == d_end), 0)\n",
    "                d_start, d_end = days[idx]\n",
    "            win_start, win_end = d_start, d_end\n",
    "\n",
    "        params = {\n",
    "            \"sort\":\"latest-published\",\n",
    "            \"limit\": str(LIMIT_PER_CALL),\n",
    "            \"offset\": str(offset),\n",
    "            \"section-ids\": SECTION_IDS,\n",
    "            \"published-after\": str(to_epoch_ms(win_start)),\n",
    "            \"published-before\": str(to_epoch_ms(win_end)-1),\n",
    "        }\n",
    "        html, code = fetch_json(sess, params)\n",
    "        if code in (403,429):\n",
    "            continue\n",
    "        if html is None:\n",
    "            continue\n",
    "        links = parse_search_links(html)\n",
    "        new_links = []\n",
    "        with STATE_LOCK:\n",
    "            seen = st[\"seen_urls\"]\n",
    "            for u in links:\n",
    "                if u not in seen:\n",
    "                    seen[u] = 1\n",
    "                    new_links.append(u)\n",
    "\n",
    "        if new_links:\n",
    "            for u in new_links:\n",
    "                try:\n",
    "                    work_q.put(u, timeout=5)\n",
    "                except queue.Full:\n",
    "                    break\n",
    "            with STATE_LOCK:\n",
    "                st[\"produced\"] += len(new_links)\n",
    "\n",
    "        got = len(links)\n",
    "        if got < LIMIT_PER_CALL:\n",
    "            with STATE_LOCK:\n",
    "                st[\"offset\"] = 0\n",
    "                if mode == \"month\":\n",
    "                    if offset >= 90*LIMIT_PER_CALL or st.get(\"force_day\", False):\n",
    "                        st[\"mode\"] = \"day\"\n",
    "                        st[\"day_cursor\"] = None\n",
    "                        st[\"force_day\"] = False\n",
    "                    else:\n",
    "                        months = month_windows_until(st[\"snapshot_utc\"], months_back=307)\n",
    "                        if month_cursor is None:\n",
    "                            next_idx = 1\n",
    "                        else:\n",
    "                            idx = next((i for i,(s,e) in enumerate(months) if e == datetime.fromisoformat(month_cursor)), 0)\n",
    "                            next_idx = idx+1\n",
    "                        if next_idx >= len(months):\n",
    "                            st[\"month_cursor\"] = None\n",
    "                            st[\"mode\"] = \"done\"\n",
    "                            break\n",
    "                        st[\"month_cursor\"] = months[next_idx][1].isoformat()\n",
    "                elif mode == \"day\":\n",
    "                    days = day_slices(m_start, m_end)\n",
    "                    if st[\"day_cursor\"] is None:\n",
    "                        cur_idx = 0\n",
    "                    else:\n",
    "                        cur_idx = next((i for i,(s,e) in enumerate(days) if e == datetime.fromisoformat(st[\"day_cursor\"])), 0)\n",
    "                    next_idx = cur_idx + 1\n",
    "                    if next_idx >= len(days):\n",
    "                        st[\"mode\"] = \"month\"\n",
    "                        st[\"day_cursor\"] = None\n",
    "                        st[\"force_day\"] = False\n",
    "                        months = month_windows_until(st[\"snapshot_utc\"], months_back=307)\n",
    "                        if month_cursor is None:\n",
    "                            next_m = months[1][1].isoformat()\n",
    "                        else:\n",
    "                            idx = next((i for i,(s,e) in enumerate(months) if e == datetime.fromisoformat(month_cursor)), 0)\n",
    "                            next_m = months[idx+1][1].isoformat() if idx+1 < len(months) else None\n",
    "                        if next_m is None:\n",
    "                            st[\"mode\"] = \"done\"\n",
    "                            break\n",
    "                        st[\"month_cursor\"] = next_m\n",
    "                    else:\n",
    "                        st[\"day_cursor\"] = days[next_idx][1].isoformat()\n",
    "        else:\n",
    "            with STATE_LOCK:\n",
    "                st[\"offset\"] = offset + LIMIT_PER_CALL\n",
    "\n",
    "def consumer_worker(work_q: queue.Queue, st: dict, idx: int):\n",
    "    sess = requests.Session()\n",
    "    out = open(ARTICLES_JL, \"a\", encoding=\"utf-8\")\n",
    "    saved_since_print = 0\n",
    "    while True:\n",
    "        try:\n",
    "            url = work_q.get(timeout=10)\n",
    "        except queue.Empty:\n",
    "            with STATE_LOCK:\n",
    "                if st.get(\"mode\") == \"done\":\n",
    "                    break\n",
    "            continue\n",
    "        try:\n",
    "            html = fetch_html(sess, url)\n",
    "            if html is None:\n",
    "                work_q.task_done()\n",
    "                continue\n",
    "            art = parse_article(html, url)\n",
    "            out.write(json.dumps(art, ensure_ascii=False) + \"\\n\"); out.flush()\n",
    "            with STATE_LOCK:\n",
    "                st[\"consumed\"] += 1\n",
    "            saved_since_print += 1\n",
    "            if st.get(\"sample_printed\") is not True:\n",
    "                print(\"\\nSAMPLE ARTICLE JSON:\\n\" + json.dumps(art, ensure_ascii=False, indent=2)[:1300] + \"...\\n\")\n",
    "                with STATE_LOCK:\n",
    "                    st[\"sample_printed\"] = True\n",
    "            if saved_since_print >= SAVE_EVERY_N:\n",
    "                with STATE_LOCK:\n",
    "                    save_state(st)\n",
    "                saved_since_print = 0\n",
    "        finally:\n",
    "            work_q.task_done()\n",
    "    out.close()\n",
    "\n",
    "def supervisor():\n",
    "    st = load_state()\n",
    "    if not st.get(\"seen_warmed\"):\n",
    "        done = load_done_set()\n",
    "        with STATE_LOCK:\n",
    "            for u in done: st[\"seen_urls\"][u] = 1\n",
    "            st[\"consumed\"] = max(st.get(\"consumed\",0), len(done))\n",
    "            st[\"seen_warmed\"] = True\n",
    "            save_state(st)\n",
    "\n",
    "    init_state_defaults(st)\n",
    "    save_state(st)\n",
    "\n",
    "    work_q = queue.Queue(maxsize=QUEUE_MAXSIZE)\n",
    "\n",
    "    prod_t = threading.Thread(target=producer, args=(work_q, st), daemon=True)\n",
    "    prod_t.start()\n",
    "\n",
    "    cons = []\n",
    "    for i in range(N_CONSUMERS):\n",
    "        t = threading.Thread(target=consumer_worker, args=(work_q, st, i), daemon=True)\n",
    "        t.start(); cons.append(t)\n",
    "\n",
    "    last_print = 0\n",
    "    try:\n",
    "        while True:\n",
    "            with STATE_LOCK:\n",
    "                mode = st.get(\"mode\")\n",
    "            if mode == \"done\" and work_q.empty():\n",
    "                break\n",
    "\n",
    "            now = time.time()\n",
    "            if now - last_print >= HEARTBEAT_EVERY_SEC:\n",
    "                line = heartbeat_line(mode, st, work_q.qsize())\n",
    "                print(\"\\r\"+line, end=\"\")\n",
    "                last_print = now\n",
    "                with STATE_LOCK: save_state(st)\n",
    "                persist_session_snapshot()\n",
    "            time.sleep(0.2)\n",
    "    finally:\n",
    "        work_q.join()\n",
    "        with STATE_LOCK:\n",
    "            save_state(st)\n",
    "        print(f\"\\nDone. State saved at: {STATE_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068420cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. State saved at: C:\\Users\\Saif\\Downloads\\prothomalo_stream_state.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the supervisor. Stop anytime; re-run to resume from state.\n",
    "try:\n",
    "    supervisor()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopped by user. State saved; safe to resume.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
